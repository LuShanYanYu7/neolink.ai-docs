---
sidebar_position: 1
title: GPU选型
sidebar_label: GPU选型
---

## 平台资源分配机制

平台分配 GPU、CPU 和内存的机制为：按租用的 GPU 数量成比例分配 CPU 和内存。算力市场显示的 CPU 和内存均为每 GPU 分配的 CPU 和内存。如果租用两卡 GPU，那么 CPU 和内存就 x2。此外，GPU 非共享，每个实例对 GPU 是独占的。

您的实例关机后如果该机器GPU被占满，再次开机时平台将为您自动调度空闲的资源。

## GPU 选型
选择合适的 GPU 型号涉及多个关键因素，包括 GPU 型号、显存大小、CPU 核数和系统内存。这些因素直接影响到计算任务的执行效率、处理能力和整体系统性能。以下是为什么在 GPU 选型时需要考虑以下这四个方面：
### GPU 型号
GPU 型号直接决定了计算单元的性能和特性，包括 CUDA 核心数、Tensor Cores（用于深度学习的专用计算单元）等。不同的型号在架构、核心数、内存带宽等方面有所不同，因此会直接影响到计算任务的执行效率和速度。Neolink.AI 平台提供了多种不同的GPU型号如4090，H20，H100，可前往[GPU型号介绍](gpuintronduction.md)或下文参考链接查看更详细的信息。
> **示例**：GeForce RTX 4090 更适合消费级图形处理和小规模深度学习任务，而 Tesla H100 则更适合于数据中心和需要大规模计算能力的深度学习和科学计算。

### 显存
显存是显卡的内存，用于存储图像、计算结果、模型参数等数据。显存容量是衡量GPU处理能力的关键指标之一，它直接影响着GPU处理数据的规模和能够支持的深度学习模型的复杂性。拥有较大显存的GPU能够一次性处理更多的数据，这意味着它们可以加载和训练更大尺寸的数据集，以及更复杂的神经网络模型。这种能力减少了在训练过程中频繁从系统内存到GPU显存的数据传输需求，从而降低了数据加载的时间开销。此外，大显存还有助于减少因显存不足而导致的训练中断和数据重载，这些都有助于显著提升模型训练的整体效率和运行速度。
> **示例**：8GB vs 24GB 显存：前者适合中小型模型和数据集，后者适合大规模深度学习任务，如训练大型神经网络或处理大型图像和视频数据集。

### CPU
CPU 对于深度学习至关重要，尽管它不直接处理模型的计算，但必须提供充足的数据处理能力来支持模型训练所需的高吞吐量。CPU 核数决定了系统同时处理任务的能力，尤其是在多线程应用程序中，可以通过并行处理提升整体计算能力。CPU 在数据预处理、模型优化和调度等方面也发挥重要作用。Neolink.AI 通常为每块GPU分配固定数量的CPU逻辑核心。理想情况下，模型计算吞吐随GPU 数量线性增长，单GPU的合理CPU逻辑核心数分配可以直接线性扩展到多GPU上。Neolink.AI 平台的算力实例提供了多种CPU分配规格。
> **示例**：多核 CPU（如 Intel Xeon 处理器）可以加速数据的预处理和后处理，提高整体系统的效率。

### 内存
系统内存的大小影响到整个系统在处理大规模数据时的稳定性和效率。足够的内存可以减少数据交换和临时存储的需求，从而提高整体计算过程的效率。
> **示例**：16GB vs 64GB 内存：前者适合一般的深度学习和科学计算任务，后者适合处理大型数据集和复杂计算任务。

## 参考链接
如果您需要了解更多，可前往以下链接查看更详细的信息。
- [NVIDIA Data Center GPU Resource Center](https://resources.nvidia.com/l/en-us-gpu)
- [TechPowerUp GPU 规格数据库](https://www.techpowerup.com/gpu-specs/)
- [英伟达A100、H100、L40S和H200简要对比](https://zhuanlan.zhihu.com/p/680317363)